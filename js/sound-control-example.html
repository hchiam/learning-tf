<!DOCTYPE html>
<html lang="en">
  <head>
    <title>tfjs-glitch-starter-howard</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style rel="stylesheet">
      * {
        background: rgb(32,32,32);
        color: whitesmoke;
        font-family: avenir, arial;
        padding: 10px;
        border-radius: 10px;
        transition: 0.5s;
        text-align: center;
      }

      body {
        font-family: avenir, arial, sans-serif;
        margin: 2em;
      }

      h1 {
        font-style: italic;
        color: #373fff;
      }

      button:hover, button:focus {
        background: lime;
        color: black;
        box-shadow: 0 2px 5px 0 white, 0 2px 10px 0 white;
      }

      legend {
        color: yellow;
      }

      #output {
        -webkit-appearance: none;
        width: 100%;
        background: #d3d3d3;
        outline: none;
        opacity: 0.7;
      }

      #output:-webkit-slider-thumb {
        -webkit-appearance: none;
        appearance: none;
        width: 25px;
        height: 25px;
        background: #4CAF50;
        cursor: pointer;
      }

      #output::-moz-range-thumb {
        width: 25px;
        height: 25px;
        background: #4CAF50;
        cursor: pointer;
      }
      
      #use-chrome {
        color: red;
      }
      
      #console {
        font-size: xx-large;
      }
    </style>
    <script src="https://unpkg.com/@tensorflow/tfjs"></script>
    <script src="https://unpkg.com/@tensorflow-models/speech-commands"></script>
  </head>  
  <body>
    <header>
      <p id="use-chrome"></p>
    </header>
    <div id="console"></div>
    <fieldset>
      <legend>Give Example Sounds to Associate with Left/Right Controls:</legend>
      <button id="left" onmousedown="collect(0)" onmouseup="collect(null)">Left</button>
      <button id="right" onmousedown="collect(1)" onmouseup="collect(null)">Right</button>
      <button id="noise" onmousedown="collect(2)" onmouseup="collect(null)">Noise</button>
    </fieldset>
    <fieldset id="train-set" style="visibility: hidden;">
      <legend>Train the Left/Right Controls on Sounds:</legend>
      <button id="train" onclick="train()">Train</button>
    </fieldset>
    <fieldset id="use" style="visibility: hidden;">
      <legend>Try Using Sounds to Control a Slider:</legend>
      <button id="listen" onclick="listen()">Listen</button>
      <input type="range" id="output" min="0" max="10" step="0.1" disabled>
    </fieldset>
    <script>
      var isChrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor);
      if (!isChrome) {
        document.getElementById('use-chrome').innerHTML = 'For best results, open this page in Chrome.';
      }

      var recognizer;

      function predictWord() {
       // Array of words that the recognizer is trained to recognize.
       const words = recognizer.wordLabels();
       recognizer.listen(({scores}) => {
         // Turn scores into a list of (score,word) pairs.
         scores = Array.from(scores).map((s, i) => ({score: s, word: words[i]}));
         // Find the most probable word.
         scores.sort((s1, s2) => s2.score - s1.score);
         document.querySelector('#console').textContent = scores[0].word;
       }, {probabilityThreshold: 0.75});
      }

      async function app() {
       recognizer = speechCommands.create('BROWSER_FFT');
       await recognizer.ensureModelLoaded();
       // predictWord();
       buildModel();
      }

      app();

      // One frame is ~23ms of audio.
      var NUM_FRAMES = 3;
      var examples = [];

      function collect(label) {
       if (recognizer.isListening()) {
         return recognizer.stopListening();
       }
       if (label == null) {
         return;
       }
       recognizer.listen(async ({spectrogram: {frameSize, data}}) => {
         let vals = normalize(data.subarray(-frameSize * NUM_FRAMES));
         examples.push({vals, label});
         document.querySelector('#console').textContent =
             `${examples.length} examples collected (to stop, click again or let go of button).`;
       }, {
         overlapFactor: 0.999,
         includeSpectrogram: true,
         invokeCallbackOnNoiseAndUnknown: true
       });
       document.getElementById('train-set').style.visibility = 'visible';
      }

      function normalize(x) {
       const mean = -100;
       const std = 10;
       return x.map(x => (x - mean) / std);
      }

      var INPUT_SHAPE = [NUM_FRAMES, 232, 1];
      var model;

      async function train() {
       toggleButtons(false);
       const ys = tf.oneHot(examples.map(e => e.label), 3);
       const xsShape = [examples.length, ...INPUT_SHAPE];
       const xs = tf.tensor(flatten(examples.map(e => e.vals)), xsShape);

       await model.fit(xs, ys, {
         batchSize: 16,
         epochs: 10,
         callbacks: {
           onEpochEnd: (epoch, logs) => {
             document.querySelector('#console').textContent =
                 `Accuracy: ${(logs.acc * 100).toFixed(1)}% Epoch: ${epoch + 1}`;
           }
         }
       });
       tf.dispose([xs, ys]);
       toggleButtons(true);
       document.getElementById('use').style.visibility = 'visible';
      }

      function buildModel() {
       model = tf.sequential();
       model.add(tf.layers.depthwiseConv2d({
         depthMultiplier: 8,
         kernelSize: [NUM_FRAMES, 3],
         activation: 'relu',
         inputShape: INPUT_SHAPE
       }));
       model.add(tf.layers.maxPooling2d({poolSize: [1, 2], strides: [2, 2]}));
       model.add(tf.layers.flatten());
       model.add(tf.layers.dense({units: 3, activation: 'softmax'}));
       const optimizer = tf.train.adam(0.01);
       model.compile({
         optimizer,
         loss: 'categoricalCrossentropy',
         metrics: ['accuracy']
       });
      }

      function toggleButtons(enable) {
       document.querySelectorAll('button').forEach(b => b.disabled = !enable);
      }

      function flatten(tensors) {
       const size = tensors[0].length;
       const result = new Float32Array(tensors.length * size);
       tensors.forEach((arr, i) => result.set(arr, i * size));
       return result;
      }

      async function moveSlider(labelTensor) {
       const label = (await labelTensor.data())[0];
       document.getElementById('console').textContent = label;
       if (label == 2) {
         return;
       }
       let delta = 0.1;
       const prevValue = +document.getElementById('output').value;
       document.getElementById('output').value =
           prevValue + (label === 0 ? -delta : delta);
      }

      function listen() {
       if (recognizer.isListening()) {
         recognizer.stopListening();
         toggleButtons(true);
         document.getElementById('listen').textContent = 'Listen';
         return;
       }
       toggleButtons(false);
       document.getElementById('listen').textContent = 'Stop';
       document.getElementById('listen').disabled = false;

       recognizer.listen(async ({spectrogram: {frameSize, data}}) => {
         const vals = normalize(data.subarray(-frameSize * NUM_FRAMES));
         const input = tf.tensor(vals, [1, ...INPUT_SHAPE]);
         const probs = model.predict(input);
         const predLabel = probs.argMax(1);
         await moveSlider(predLabel);
         tf.dispose([input, probs, predLabel]);
       }, {
         overlapFactor: 0.999,
         includeSpectrogram: true,
         invokeCallbackOnNoiseAndUnknown: true
       });
      }
    </script>
  </body>
</html>
