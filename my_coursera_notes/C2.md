# Convolutional Neural Networks in TensorFlow

- course: https://www.coursera.org/learn/convolutional-neural-networks-tensorflow

- slides: https://community.deeplearning.ai/t/tf1-course-1-lecture-notes/124222

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C2

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-2/80

- multiple layers of convolution be thought of as distilling information contributing to determining an image class

## 1) data cleaning

- **data cleaning**: handle inconsistent image sizes, other things in the images, converting to numbers in a normalized 0-1 range, etc.

  - example code/colab: https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C2/W1/ungraded_lab/C2_W1_Lab_1_cats_vs_dogs.ipynb

  - my colab assignment solution: https://colab.research.google.com/drive/1brqWrx-6qf9Pg3cnxy0uygwV5kjELPZF (see a later colab for basically the same code with data augmentation added to avoid overfitting)

    - remember: for a 2-class (binary) classification output (0% - 100% probability of being class A instead of class B), you can use `activation='sigmoid'` for the output layer, and `loss='binary_crossentropy'`

      - but for multi-class, see notes below

    - `os.path.join` and `os.makedirs`

    - convert colour ranges from 0-255 to 0-1 using `data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)` (for training - for model usage, you might be able to just do something like `x = tensorflow.keras.utils.img_to_array(img) / 255`)

    - `training_data_generator = data_generator.flow_from_directory(dir,batch_size,class_mode='binary',target_size=(150,150))`

    - this colab also gives example code for visualizing each layer to see intermediate feature representations (by using the original model to create another model that is able to provide us the multiple outputs (per-layer outputs)) - note `display_grid`

    - this colab also has code for tracking learning history, which can be used to check for overfitting (training versus validation loss) - consider stopping after 2 epochs of training? but even better: do data augmentation (see next section) and dropout (see next next section) --> a good signal that we'er avoiding overfitting is the training and testing history are increasing in sync with each other

## 2) data generation/augmentation

- **data generation/augmentation**: e.g. "image augmentation", that is, to generate images that are rotated, skewed, etc. so you can try to cover more possible cases when you have not enough data (example: images of only cats standing up vs sideways lying down) - `ImageDataGenerator(...other options!...)`

  - it's actually better to let tensorflow do this on the fly (instead of using up storage faster with making actual copies of images, and to start with unchanged data)

  - make sure that you apply data augmentation to both training and testing sets

  - (you could also generate data with realistic computer graphics CGI)

  - my colab assignment solution: https://colab.research.google.com/drive/1_lpUhJmctFI8ARWx0Da7SSyVMxqLE15s (basically the same code as before but with data augmentation added to avoid overfitting)

## 3) transfer learning

- **transfer learning** lets you use pre-existing models that were pre-trained on lots of data and extracted many features, lock the earlier layers of those models (`base_model.trainable = False` or `for layer in pre_trained_model.layers: layer.trainable = False`), and then add your own layers (`Model(inputs=pre_trained_model.input, outputs=x_extra_layers)`), to let you train faster and also get better accuracy for your specific smaller dataset/application

  - https://www.tensorflow.org/tutorials/images/transfer_learning

  - my colab assignment solution: https://colab.research.google.com/drive/1IH4JJXm4EGbtlBJLtjRhEzSS8SdU0hpa

    - uses `x = layers.Dropout(0.3)(x)`:

- **dropout**: pretend a random number of neurons are removed, to prevent neighbouring neurons from affecting each other's weights and reduce "specialization" and to prevent "over-reliance" on any one neuron (you can also change the probabilities per layer) = [regularization](https://github.com/hchiam/machineLearning/blob/master/more_notes/googleMLCrashCourse.md#:~:text=regularization%20%3D%20penalizing%20model%20complexity)

  - more details: https://www.youtube.com/watch?v=ARq74QuavAo

## 4) multi-class classification

- my colab assignment solution: https://colab.research.google.com/drive/1PgX7dF6e7FF9f14Gff81FeiE3CgZ-vHC

- for a 2-class (binary) classification output (0% - 100% probability of being class A instead of class B), you can use `activation='sigmoid'` for the output layer, and `loss='binary_crossentropy'`

- but for multi-class, use `class_mode='categorical'` for the data generator, use `number_of_classes, activation='softmax'` for the output layer, and `loss='categorical_crossentropy'`

- `categorical_crossentropy` vs `sparse_categorical_crossentropy` = one-hot and all probabilities vs just one: ([reference](https://stackoverflow.com/questions/58565394/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c/58566065#58566065))

  - `categorical_crossentropy` gives you `[0, 1, 0, 0, 0]` and `[.2, .5, .1, .1, .1]`
  - `sparse_categorical_crossentropy` gives you `[1]` and `[.5]`
