# Sequences, Time Series, and Prediction in TensorFlow

- course: https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction

- slides: https://community.deeplearning.ai/t/tf1-course-4-lecture-notes/121711

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C4 - Jupyter notebooks not rendering for you on GitHub? Try https://nbviewer.org

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-4/82

- **univariate** time series = one variable over time
- **multivariate** time series = multiple variables over time, but together are more helpful

- **imputed** data = filled-in data that was missing (e.g. forecasting "backwards" into the past, or to fill in gaps in the data kinda like interpolation)

- **real-word signals** you'll encounter will likely have a combination of:

  1. **trend** (global pattern, btw which can extend forward/backwards in time),
  2. **seasonality** (local pattern),
  3. **auto-correlation** (correlation with a delayed copy of itself),
  4. **noise** ("truly" unpredictable), and
  5. **non-stationary behaviour** (behaviour may change, so you might want to give your model only a "window" of the data)

- **fixed partitioning**:

  - consider splitting time series data into training/validation/test periods with whole numbers of seasonality "seasons" in each period.
  - Use training data vs validation data, then combine (training + validation) data vs test data. Why? test data = typically closest to current data, or is just future data

- **roll-forward partitioning**:

  - repeated fixed-partitioning with increasing size of training period to forecast next day in the validation period (i.e. in the validation data)

- metrics for evaluating sequence/time-series model perf:

  - `errors = forecasts - actual`
  - `mse = np.square(errors).mean()` = Mean Squared Error, squared to get rid of negative values and avoid positive/negative errors cancelling each other out.
  - `rmse = np.sqrt(mse)` = Root Mean Squared Error, for same scale of original errors.
  - `mae = np.abs(errors).mean()` = Mean Absolute Error (aka MAD for Mean Absolute Deviation), if you want to avoid penalizing large errors as much as MSE does.
    - `keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy()`
  - `mape = np.abs(errors / x_valid).mean()` = Mean Absolute Percentage Error, to give an idea of the size of the errors compared to the size of the values.

- **before you try deep learning, simpler approaches might work**:

  - **moving average** eliminates noise and tries to guess at local trend but can't capture seasonality:
    - **trailing window** for moving average of **present** values, e.g. of differenced series (see below).
    - **centered window** for moving average of **past** values, and can be more accurate, but can only be used for past values (can't get future values).
  - **differencing** eliminates seasonality and trend, and instead compares a time with a previous time, e.g. `series(t) - series(t - 1yearAgo)` or in Python-specific synax: `(series[365:] - series[:-365])`.
  - **moving average of differenced time series** = combine the 2 previous concepts: use differencing, then use a moving average on that, then add back `moving_average_of( series(t - 1yearAgo) )` to get back an estimate of the original time series `series(t)` with restored seasonality and trend, or rather add back the moving average of `series(t - 1yearAgo)` for less noise and to effectively use 2 moving averages: `moving_average(series[split_time - 365 - window//2:-365 + window//2], window) + moving_average( (series[365:] - series[:-365]) , window )`.
    - example: https://colab.research.google.com/drive/1ZQQ-RgCczaMs7OPt_HoXIFlltcPLL3Hz#scrollTo=eTD4ATkFYNZp

## reading csv with headers

```py
import csv
time_step = []
sunspots = []

with open('/path/to/file.csv') as csvfile:
  reader = csv.reader(csvfile, delimiter=',')
  next(reader) # ignore header row
  for i, row in enumerate(reader):
    # time_step.append(i)
    time_step.append(int(row[0]))
    sunspots.append(float(row[2]))
# convert np.array at end for better perf, otherwise appending to np.array is slow
time_step = np.array(time_step)
sunspots = np.array(sunspots)
```

## lambda layers

Lambda layers let you run arbitrary code/functions within the model definition itself.

## DNN (Deep Neural Network)

Similar to NLP examples in [C3.md](https://github.com/hchiam/learning-tf/blob/master/my_coursera_notes/C3.md):

- input "features" = x previous values
- label = next value

When you use RNN recurrent layers, the input window shape needs to be 3D:

- `shape=[ batch size, # of time steps for RNN, # of (multivariate) dimensions ]`

- **sequence-to-sequence RNN** outputs per time step = sequences (`return_sequences=True, input_shape=[None, 1]`, especially if passing along to another RNN layer) = batch of sequences, given a batch of sequences
- **sequence-to-vector RNN** is just ignoring all time step outputs except the last one (default `tf.keras` behaviour) = batch of single outputs, given a batch of sequences

If you notice lots of "irregular wobbling" (technically called noise or instability) in the training loss curve over epochs or the training MAE curve over epochs:

- try increasing (or decreasing) batch size
- if the noise/instability seems to have regular intervals, then the batch size might be only slightly off what it should be (lower batch size slightly?)
- try making window size and batch size evenly divide up the training data size

```py
# TIP: try tuning these values but also model values and design:

window_size = 60
batch_size = 100 # TIP: try increasing (or decreasing) batch size to see if that reduces overfitting or reduces noise/instability in the decreasing loss or MAE curves
shuffle_buffer_size = 1000
split_time = 3000 # and 500 for validation
num_epochs = 100 # at first to get an idea of a good learning rate, then change to 500

initial_learning_rate = 1e-8 # might automatically find something better like 1e-5
optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.9)

# will use this and model.fit history to plot/find best (and stable) learning rate:
learning_rate_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(
  lambda epoch: 1e-8 * 10**(epoch/20) # make learning rate increase with epoch number
)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer_size):
  dataset = tf.data.Dataset.from_tensor_slices(series)
  return dataset.window(size=window_size + 1, shift=1, drop_remainder=True) # drop_remainder option is confusingly named: makes return only datasets of window size
    .flat_map(lambda window: window.batch(window_size + 1))
    .shuffle(buffer_size=shuffle_buffer_size) # buffer_size for faster perf
    .map(lambda window: (window[:-1], window[-1])) # (features, labels) (last as labels)
    .batch(batch_size=batch_size).prefetch(1)

# data:
dataset = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)

# model:
tf.keras.backend.clear_session() # so model doesn't affect later versions of itself
# layer1 = tf.keras.layers.Dense(1, input_shape=[window_size]) # so we can layer1.get_weights()
model = tf.keras.models.Sequential([
  # layer1,

  # tf.keras.layers.Dense(1)

  # tf.keras.layers.SimpleRNN(40, return_sequences=True), #, input_shape=[None, 1]),
  # tf.keras.layers.SimpleRNN(40),

  # tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]), #2D + 1D at index -1 = 3D
  tf.keras.layers.Conv1D( # CNN layer
      filters=60,
      kernel_size=5, # 5-number window convolution
      strides=1,
      padding='causal',
      activation='relu',
      input_shape=[None, 1] # reshape to work with the later univariate LSTM layer
  ),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True)), # LSTM layer
  # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60)),
  tf.keras.layers.Dense(30, input_shape=[window_size], activation='relu'),
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400) # scale up tanh [-1,+1] to same range as data [1,400]
])
model.compile(
  # loss='mse',
  loss=tf.keras.losses.Huber(), # Huber loss function is less sensitive to outliers than, say, MSE is to outliers - https://en.wikipedia.org/wiki/Huber_loss
  optimizer=optimizer,
  metrics=['mae']
)
history = model.fit(
  dataset,
  epochs=num_epochs,
  callbacks=[learning_rate_scheduler_callback]
)

# predict:
# model.predict(series[1:21][np.newaxis]) # np.newaxis auto-reshapes to model's input dimension
# or:
forecast = []
for time in range(len(series) - window_size):
  forecast.append(model.predict( series[time:time+window_size][np.newaxis] ))
forecast = forecast[split_time - window_size : ] # get predictions after the split for validation
results = np.array(forecast)[:, 0, 0]

# plot learning rate versus loss (to get an idea of the best learning rate to use):
learning_rate_series = 1e-8 * (10**(np.range(100) / 20))
# import matplotlib.pyplot as plt
plt.semilogx(learning_rate_series, history.history['loss'])
plt.axis([1e-8, 1e-3, 0, 300])

# plot loss over time for one given learning rate:
ignore_first_few = 10
loss_history = history.history['loss']
epochs = range(ignore_first_few, len(loss_history))
plt.plot(epochs, loss_history[ignore_first_few:], 'b', label='training loss')
plt.show()

print(tf.keras.metrics.mean_absolute_error(x_validation, results).numpy())
```

## Further self-directed learning: use a transformer instead of RNN?

Read this tutorial: https://www.tensorflow.org/text/tutorials/transformer

## Further learning

Save model at checkpoints of every 10 epoch and saving only the best val_accuracy:

```py
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="mymodel.h5",
    monitor='val_accuracy',
    mode='max',
    verbose=2,
    save_freq='epoch',
    period=10,
    save_best_only=True)

history = model.fit(ds_train, epochs=200, verbose=2,
                    validation_data=(ds_validation,),
                    callbacks=[model_checkpoint_callback])
```

Deploy: https://www.coursera.org/specializations/tensorflow-data-and-deployment

- [JS](https://www.coursera.org/learn/browser-based-models-tensorflow?specialization=tensorflow-data-and-deployment)
- [lite](https://www.coursera.org/learn/device-based-models-tensorflow?specialization=tensorflow-data-and-deployment)
- data pipelines/services

Advanced: https://www.coursera.org/specializations/tensorflow-advanced-techniques

- custom
- distributed
- image segmentation, object localization
- generative, transfer

## Links to other notes

https://github.com/hchiam/learning-tf/tree/main/my_coursera_notes

https://github.com/hchiam/learning-tf/blob/main/my_coursera_notes/C2.md

https://github.com/hchiam/learning-tf/blob/main/my_coursera_notes/C3.md
