# Sequences, Time Series, and Prediction in TensorFlow

- course: https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction

- slides: https://community.deeplearning.ai/t/tf1-course-4-lecture-notes/121711

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C4 - Jupyter notebooks not rendering for you on GitHub? Try https://nbviewer.org

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-4/82

- **univariate** time series = one variable over time
- **multivariate** time series = multiple variables over time, but together are more helpful

- **imputed** data = filled-in data that was missing (e.g. forecasting "backwards" into the past, or to fill in gaps in the data kinda like interpolation)

- **real-word signals** you'll encounter will likely have a combination of:

  1. **trend** (global pattern, btw which can extend forward/backwards in time),
  2. **seasonality** (local pattern),
  3. **auto-correlation** (correlation with a delayed copy of itself),
  4. **noise** ("truly" unpredictable), and
  5. **non-stationary behaviour** (behaviour may change, so you might want to give your model only a "window" of the data)

- **fixed partitioning**:

  - consider splitting time series data into training/validation/test periods with whole numbers of seasonality "seasons" in each period.
  - Use training data vs validation data, then combine (training + validation) data vs test data. Why? test data = typically closest to current data, or is just future data

- **roll-forward partitioning**:

  - repeated fixed-partitioning with increasing size of training period to forecast next day in the validation period (i.e. in the validation data)

- metrics for evaluating sequence/time-series model perf:

  - `errors = forecasts - actual`
  - `mse = np.square(errors).mean()` = Mean Squared Error, squared to get rid of negative values and avoid positive/negative errors cancelling each other out.
  - `rmse = np.sqrt(mse)` = Root Mean Squared Error, for same scale of original errors.
  - `mae = np.abs(errors).mean()` = Mean Absolute Error (aka MAD for Mean Absolute Deviation), if you want to avoid penalizing large errors as much as MSE does.
    - `keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy()`
  - `mape = np.abs(errors / x_valid).mean()` = Mean Absolute Percentage Error, to give an idea of the size of the errors compared to the size of the values.

- **before you try deep learning, simpler approaches might work**:

  - **moving average** eliminates noise and tries to guess at local trend but can't capture seasonality:
    - **trailing window** for moving average of **present** values, e.g. of differenced series (see below).
    - **centered window** for moving average of **past** values, and can be more accurate, but can only be used for past values (can't get future values).
  - **differencing** eliminates seasonality and trend, and instead compares a time with a previous time, e.g. `series(t) - series(t - 1yearAgo)` or in Python-specific synax: `(series[365:] - series[:-365])`.
  - **moving average of differenced time series** = combine the 2 previous concepts: use differencing, then use a moving average on that, then add back `moving_average_of( series(t - 1yearAgo) )` to get back an estimate of the original time series `series(t)` with restored seasonality and trend, or rather add back the moving average of `series(t - 1yearAgo)` for less noise and to effectively use 2 moving averages: `moving_average(series[split_time - 365 - window//2:-365 + window//2], window) + moving_average( (series[365:] - series[:-365]) , window )`.
    - example: https://colab.research.google.com/drive/1ZQQ-RgCczaMs7OPt_HoXIFlltcPLL3Hz#scrollTo=eTD4ATkFYNZp

## DNN (Deep Neural Network)

Similar to NLP examples in [C3.md](https://github.com/hchiam/learning-tensorflow/blob/master/my_coursera_notes/C3.md):

- input "features" = x previous values
- label = next value

```py
window_size = 20
batch_size = 32
shuffle_buffer_size = 1000
# split_time = ...

def windowed_dataset(series, window_size, batch_size, shuffle_buffer_size):
  dataset = tf.data.Dataset.from_tensor_slices(series)
  return dataset.window(size=window_size + 1, shift=1, drop_remainder=True) # drop_remainder option is confusingly named: makes return only datasets of window size
    .flat_map(lambda window: window.batch(window_size + 1))
    .shuffle(buffer_size=shuffle_buffer_size) # buffer_size for faster perf
    .map(lambda window: window[:-1], window[-1]) # features and labels (last as labels)
    .batch(batch_size=batch_size).prefetch(1)

# data:
dataset = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)

# modeL:
# layer1 = tf.keras.layers.Dense(1, input_shape=[window_size]) # so we can layer1.get_weights()
model = tf.keras.models.Sequential([
  # layer1,
  tf.keras.layers.Dense(10, input_shape=[window_size], activation='relu'),
  tf.keras.layers.Dense(10, activation='relu'),
  tf.keras.layers.Dense(11)
])
model.compile(
  loss='mse',
  optimzer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9)
)
model.fit(dataset, epochs=100)

# predict:
# model.predict(series[1:21][np.newaxis]) # np.newaxis auto-reshapes to model's input dimension
# or:
forecast = []
for time in range(len(series) - window_size):
  forecast.append(model.predict( series[time:time+window_size][np.newaxis] ))
forecast = forecast[split_time - window_size : ] # get predictions after the split for validation
results = np.array(forecast)[:, 0, 0]
```
