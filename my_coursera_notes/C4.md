# Sequences, Time Series, and Prediction in TensorFlow

- course: https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction

- slides: https://community.deeplearning.ai/t/tf1-course-4-lecture-notes/121711

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C4 - Jupyter notebooks not rendering for you on GitHub? Try https://nbviewer.org

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-4/82

- **univariate** time series = one variable over time
- **multivariate** time series = multiple variables over time, but together are more helpful

- **real-word signals** you'll encounter will likely have a combination of:

  1. **trend** (global pattern, btw which can extend forward/backwards in time),
  2. **seasonality** (local pattern),
  3. **auto-correlation** (correlation with a delayed copy of itself),
  4. **noise** ("truly" unpredictable), and
  5. **non-stationary behaviour** (behaviour may change, so you might want to give your model only a "window" of the data)

- **fixed partitioning**:

  - consider splitting time series data into training/validation/test periods with whole numbers of seasonality "seasons" in each period.
  - Use training data vs validation data, then combine (training + validation) data vs test data. Why? test data = typically closest to current data, or is just future data

- **roll-forward partitioning**:

  - repeated fixed-partitioning with increasing size of training period to forecast next day in the validation period (i.e. in the validation data)

- metrics for evaluating sequence/time-series model perf:

  - `errors = forecasts - actual`
  - `mse = np.square(errors).mean()` = Mean Squared Error, squared to get rid of negative values and avoid positive/negative errors cancelling each other out.
  - `rmse = np.sqrt(mse)` = Root Mean Squared Error, for same scale of original errors.
  - `mae = np.abs(errors).mean()` = Mean Absolute Error (aka MAD for Mean Absolute Deviation), if you want to avoid penalizing large errors as much as MSE does.
    - `keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy()`
  - `mape = np.abs(errors / x_valid).mean()` = Mean Absolute Percentage Error, to give an idea of the size of the errors compared to the size of the values.

- **before you try deep learning, simpler approaches might work**:

  - **moving average** eliminates noise and tries to guess at local trend but can't capture seasonality:
    - **trailing window** for moving average of **present** values, e.g. of differenced series (see below).
    - **centered window** for moving average of **past** values, and can be more accurate, but can only be used for past values (can't get future values).
  - **differencing** eliminates seasonality and trend, and instead compares a time with a previous time, e.g. `series(t) - series(t - 1yearAgo)` or in Python-specific synax: `(series[365:] - series[:-365])`.
  - **moving average of differenced time series** = combine the 2 previous concepts: use differencing, then use a moving average on that, then add back `moving_average_of( series(t - 1yearAgo) )` to get back an estimate of the original time series `series(t)` with restored seasonality and trend, or rather add back the moving average of `series(t - 1yearAgo)` for less noise and to effectively use 2 moving averages: `moving_average(series[split_time - 365 - window:-365 + window], window) + moving_average( (series[365:] - series[:-365]) , window )`.
    - example: https://colab.research.google.com/drive/1ZQQ-RgCczaMs7OPt_HoXIFlltcPLL3Hz#scrollTo=eTD4ATkFYNZp
