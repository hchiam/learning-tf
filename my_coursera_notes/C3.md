# Natural Language Processing in TensorFlow

- course: https://www.coursera.org/learn/natural-language-processing-tensorflow

- slides: https://community.deeplearning.ai/t/tf1-course-1-lecture-notes/124222

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C3 - Jupyter notebooks not rendering for you on GitHub? Try https://nbviewer.org

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-3/81

- to visualize embeddings (.tsv files): https://projector.tensorflow.org/ --> Load data --> Sphereize data (for binary clustering of the data)

  - or use TensorBoard locally: (see notes on Python code you must run first, see below for visualization with TensorBoard) and then `tensorboard --logdir logs` and open http://localhost:6006/#projector

- sarcasm detection dataset: https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home

- IMDB review dataset: https://ai.stanford.edu/~amaas/data/sentiment/

- stopwords = words to ignore for our purposes (e.g. "of", "the")

- fun fact: `tensorflow_datasets` TensorFlow Data Services (TFDS) are built-in providers of datasets so you can get started learning ML faster

  ```py
  import tensorflow_datasets as tfds
  import numpy as np
  imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
  train_data, test_data = imdb['train'], imdb['test']
  train_labels = []
  train_sentences = []
  test_labels = []
  test_sentences = []
  for sentence, label in train_data:
    train_labels.append(label.numpy()) # .numpy() turns tensor into value(s)
    train_sentences.append(sentence.numpy().decode('utf8'))
  for sentence, label in test_data:
    test_labels.append(label.numpy())
    test_sentences.append(sentence.numpy().decode('utf8'))
  train_labels_final = np.array(train_labels) # convert np.array at end for better perf
  test_labels_final = np.array(test_labels) # otherwise appending to np.array is slow
  ```

  - more: https://github.com/tensorflow/datasets/tree/master/docs/catalog and https://www.tensorflow.org/datasets/catalog/overview

## load JSON data into Python:

```py
import json

with open('data.json', 'r') as f:
  datastore = json.load(f)
  # print(f'First line" {f.readline()}')
  # print(f'Second line" {f.readline()}')

  prop1 = []
  prop2 = []
  prop3 = []
  for row in datastore:
    prop1.append(row['prop1'])
    prop2.append(row['prop2'])
    prop3.append(row['prop3'])
```

## load CSV data into Python:

```py
import csv

with open('data.csv', 'r') as f:
  datastore = csv.reader(csvfile, delimiter=',')
  # print(f'First line" {f.readline()}')
  # print(f'Second line" {f.readline()}')

  prop1 = []
  prop2 = []
  prop3 = []
  for row in datastore:
    prop1.append(row['prop1'])
    prop2.append(row['prop2'])
    prop3.append(row['prop3'])
```

```py
# hyperparameters:
vocab_size = 100
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'
num_epochs = 10

sentences = ['I love my dog', 'I love my cat']

import tensorflow as tf
from tf import keras
from tf.keras.preprocessing.text import Tokenizer
from tf.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(
  num_words=vocab_size, # use top x most-common words to tokenize SENTENCES for tons of data
  oov_token=oov_tok, # mark not-yet-seen words as OOV = Out Of Vocabulary
)

tokenizer.fit_on_texts(sentences)

word_index = tokenizer.word_index # dictionary key-value pairs (may have len > than num_words)
print(word_index) # {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}

sequences = tokenizer.texts_to_sequences(sentences)
print(sequences) # [[1,2,3,4], [1,2,3,5]]

padded = pad_sequences( # add 0s to make inputs of uniform size
  sequences,
  padding='post', # use 'post' to add padding to end of sentence (default is 'pre')
  # maxlen=max_length, # max number of words to include in encoding (else all len of longest)
  # truncating=trunc_type # truncate from end of sentence if there too many words in sentence
)
# do similar for test: test_padded = pad_sequences(...)
test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=Max_length)
```

## create a model with an embedding layer!

```py
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  # tf.keras.layers.Flatten(), # may be slower but more accurate than:
  tf.keras.layers.GlobalAveragePooling1D(), # flatten by averaging across the vector
  tf.keras.layers.Dense(6, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
history = model.fit(
  padded,
  train_labels_final,
  epochs=num_epochs,
  validation_data=(test_padded, test_labels_final)
)
```

```py
import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_' + string])
  plt.xlabel('Epochs')
  plt.ylabel(string)
  plt.legend([string, 'val_' + string])
  plt.show()

plot_graphs(history, 'acc')
plot_graphs(history, 'loss')
```

Note: often for text data, you might notice validation (AKA test) accuracy increase but its loss increase = slowly more accurate predictions over time but confidence per prediction decreased. Try playing with hyperparameters to see what performs best.

## visualize the model's embeddings

```py
embedding_layer = model.layers[0] # first layer as per model above
weights = embedding_layer.get_weights()
print(weights.shape) # shape: (vocab_size, embedding_dim), e.g. (10000, 16)
print(tokenizer.word_index) # e.g., {word: 1, ...}
print(tokenizer.index_word) # e.g., {1: word, ...}
index_word = tokenizer.index_word

import io
# vectors = embeddings for each word
# words = "metadata" (labels/words represented by those embeddings)
vectors = io.open('vectors.tsv', 'w', encoding='utf-8')
words = io.open('words.tsv', 'w', encoding='utf-8')
for word_number_key in range(1, vocab_size):
  word = index_word[word_number_key] # index_word as opposed to word_index
  embeddings = weights[word_number_key]
  words.write(word + '\n')
  vectors.write('\t'.join([str(x) for x in embeddings]) + '\n')
vectors.close()
words.close()

# # to download the files from Google Colab to your computer:
# try:
#   from google.colab import files
# except ImportError:
#   pass
# else:
#   files.download('vectors.tsv')
#   files.download('words.tsv')
```

## to then visualize those exported .tsv files:

https://projector.tensorflow.org/ --> Load data --> Sphereize data (for binary clustering of the data)

or use TensorBoard locally: (include the following Python code when running the model) and then `tensorboard --logdir logs/imdb-example` and http://localhost:6006/#projector but you need to first create a checkpoint while running your model!

```py
# this is a slightly tweaked version of the code in the documentation: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin

from tensorboard.plugins import projector

log_dir = 'logs/imdb-example/' # DON'T include a "/" before "logs" otherwise get misleading "read-only" error message
if not os.path.exists(log_dir):
    os.makedirs(log_dir) # this doesn't seem to work in PyCharm

# Save the weights we want to analyze as a variable. Note that the first
# value represents any unknown word, which is not in the metadata, here
# we will remove this value.
weights_as_var = tf.Variable(model.layers[0].get_weights()[0][1:])
# Create a checkpoint from embedding, the filename and key are the
# name of the tensor.
checkpoint = tf.train.Checkpoint(embedding=weights_as_var)
checkpoint.save(os.path.join(log_dir, "embedding.ckpt"))

# Set up config.
config = projector.ProjectorConfig()
embedding = config.embeddings.add()
# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.
embedding.tensor_name = "embedding/.ATTRIBUTES/VARIABLE_VALUE"
embedding.metadata_path = 'metadata.tsv'
projector.visualize_embeddings(log_dir, config)
```

## RNN and LSTM to take word order into account

RNN = pass info to next time step (GRU and LSTM are example types of RNN)

LSTM = Long Short-Term Memory = remember some info from even far-away previous time steps, or even affect past time steps

GRU = Gated Recurrent Unit

## LSTM code

```py
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # <-- LSTM with 64 outputs
  # Bidirectional means cell state can go in both directions, as mentioned above, and doubles the size of the layer
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
```

## stacking LSTMs can be better by smoothening accuracy over epochs

```py
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
  # need return_sequences=True to make sure the output size is next input size
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
```

## combining LSTM with previous concepts

- make sure to test accuracy and validation accuracy over epochs
- note if the validation accuracy is same or better than without LSTM
- note that if the validation accuracy or loss are getting worse, there may be overfitting potentially (especially with a small dataset and lots of OOV words in the validation set), so try tweaking the parameters or layers

```py
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),

  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)), # try simpler first!
  # # or:
  # tf.keras.layers.Bidirectional(tf.keras.layers.GRU(10)),
  # # or:
  # tf.keras.layers.Conv1D(128, 5, activation='relu'),
  # tf.keras.layers.GlobalAveragePooling1D(), # or tf.keras.layers.GlobalMaxPooling1D(),

  tf.keras.layers.Dense(6, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
# history = model.fit(...)
```

## generating text with an RNN

https://www.tensorflow.org/text/tutorials/text_generation

Instead of generating a word from thin air, predict the next word, based on a corpus of example sentences to have a sense of the next expected word in a sentence. Kinda like a weather forecast, with decreasing confidence in the words further along the sentence into the "future".

```py
import tensorflow as tf
import numpy as np
from tf.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tf.keras.models import Sequential
from tf.keras.preprocessing.text import Tokenizer
from tf.keras.preprocessing.sequence import pad_sequences
```

### prep data for generating text with an RNN

```py
# data = 'corpus as a single string for now\nwow this is a simple example\n...'
data = open('./data.txt').read()
corpus = data.lower().split('\n')

tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus) # kvp: key = word, value = token for that word
total_words = len(tokenizer.word_index) + 1 # +1 for OOV words

# input_sequences will be a 2D array: a list of sequences of tokens
input_sequences = [] # will pad and then split into x's and y's (labels / next words)
# use corpus sentences to add to input_sequences:
for line in corpus:
  token_list = tokenizer.texts_to_sequences([line])[0] # e.g. 'A quick ...' --> [4 2 ...]
  for i in range(1, len(token_list)):
    # use i to generate n-grams, all starting from the start of the sentence:
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

# get length of longest sentence in corpus to then pad all sentences for uniform input length:
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(
  input_sequences,
  maxlen=max_sequence_len,
  padding='pre'
))
# (use PRE-padding so it's easier to get the label = it's the last token!)
# [0 0  4  2]: input is 0 0 4,  output label is  2
# [0 4  2 66]: input is 0 4 2,  output label is 66
# [4 2 66  8]: input is 4 2 66, output label is  8

xs, labels = input_sequences[:, :-1], input_sequences[:, -1]
# e.g. [4 2 66  8] --> x = [4 2 66], label = 8

ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)
# to_categorical converts labels to one-hot-encodings with 1 corpus word slot filled
```

### neural network for generating text with an RNN

```py
# from tensorflow.keras.optimizers import Adam

embedding_dimensions = 100
adam = Adam(learning_rate=0.005) # is 0.01 too high? lower a little to avoid "bouncing"

model = Sequential([
    Embedding(
      total_words,
      embedding_dimensions,
      input_length=max_sequence_len - 1 # -1 because final word = label
    ),
    Bidirectional(LSTM(200)), # Bidirectional = carry context backwards too
    Dense(total_words, activation='softmax')
])
model.compile(
  loss='categorical_crossentropy',
  optimizer=adam,
  metrics=['accuracy']
)
mode.summary()
model.fit(xs, ys, epochs=50)
# use 500 epochs when it takes longer to converge because very little data
```

```py
seed_text = 'Laurence went to dublin'
next_few_words = 10

def continued_sentence(seed_text, next_few_words):
  for _ in range(next_few_words):
    # convert seed text to token list:
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    # pad token list to match uniform input length of token_list like earlier:
    token_list = pad_sequences(
      [token_list],
      maxlen=max_sequence_len - 1,
      padding='pre'
    )
    # probabilities = model.predict(token_list)
    # predicted_next_token = np.argmax(probabilities, axis=-1)[0]
    # if predicted != 0:
    #   predicted_next_word = tokenizer.index_word[predicted_next_token]
    # seed_text += ' ' + predicted_next_word
    predicted_next_token = model.predict_classes(token_list, verbose=0)
    predicted_next_word = tokenizer.index_word[predicted_next_token]
    seed_text += ' ' + predicted_next_word
  return seed_text

print(continued_sentence(seed_text, next_few_words))
```

- my colab assignment solution: https://colab.research.google.com/drive/1H5Z0-cbwxQC7ApMtsH9oznJWnqsf0nsM
