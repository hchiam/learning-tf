# Natural Language Processing in TensorFlow

- course: https://www.coursera.org/learn/natural-language-processing-tensorflow

- slides: https://community.deeplearning.ai/t/tf1-course-1-lecture-notes/124222

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C3

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-3/81

- sarcasm detection dataset: https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home

- stopwords = words to ignore for our purposes (e.g. "of", "the")

```py
import tensorflow as tf
from tf import keras
from tf.keras.preprocessing.text import Tokenizer
from tf.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(
  num_words=100, # use top 100 most-common words to tokenize SENTENCES for tons of data
  oov_token='<OOV>', # mark not-yet-seen words as OOV = Out Of Vocabulary
)

sentences = ['I love my dog', 'I love my cat']
tokenizer.fit_on_texts(sentences)

word_index = tokenizer.word_index # dictionary key-value pairs (may have len > than num_words)
print(word_index) # {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}

sequences = tokenizer.texts_to_sequences(sentences)
print(sequences) # [[1,2,3,4], [1,2,3,5]]

padded = pad_sequences( # add 0s to make inputs of uniform size
  sequences,
  padding='post', # use 'post' to add padding to end of sentence (default is 'pre')
  # maxlen=5, # max number of words to include in encoding (else all len of longest)
  # truncating='post' # truncate from end of sentence if there too many words in sentence
)
```

```py
# load JSON data in Python:

import json

with open('data.json', 'r') as f:
  datastore = json.load(f)

prop1 = []
prop2 = []
prop3 = []
for item in datastore:
  prop1.append(item['prop1'])
  prop2.append(item['prop2'])
  prop3.append(item['prop3'])
```
