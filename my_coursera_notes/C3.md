# Natural Language Processing in TensorFlow

- course: https://www.coursera.org/learn/natural-language-processing-tensorflow

- slides: https://community.deeplearning.ai/t/tf1-course-1-lecture-notes/124222

- companion repo: https://github.com/https-deeplearning-ai/tensorflow-1-public/tree/main/C3

- discussion forum: https://community.deeplearning.ai/c/tf1/tf1-course-3/81

- to visualize embeddings (.tsv files): https://projector.tensorflow.org/ --> Load data --> Sphereize data (for binary clustering of the data)

- sarcasm detection dataset: https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/home

- IMDB review dataset: https://ai.stanford.edu/~amaas/data/sentiment/

- stopwords = words to ignore for our purposes (e.g. "of", "the")

- fun fact: `tensorflow_datasets` TensorFlow Data Services (TFDS) are built-in providers of datasets so you can get started learning ML faster

  ```py
  import tensorflow_datasets as tfds
  import numpy as np
  imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
  train_data, test_data = imdb['train'], imdb['test']
  train_labels = []
  train_sentences = []
  test_labels = []
  test_sentences = []
  for sentence, label in train_data:
    train_labels.append(label.numpy()) # .numpy() turns tensor into value(s)
    train_sentences.append(sentence.numpy().decode('utf8'))
  for sentence, label in test_data:
    test_labels.append(label.numpy())
    test_sentences.append(sentence.numpy().decode('utf8'))
  train_labels_final = np.array(train_labels)
  test_labels_final = np.array(test_labels)
  ```

  - more: https://github.com/tensorflow/datasets/tree/master/docs/catalog and https://www.tensorflow.org/datasets/catalog/overview

## load JSON data into Python:

```py
import json

with open('data.json', 'r') as f:
  datastore = json.load(f)
  # print(f'First line" {f.readline()}')
  # print(f'Second line" {f.readline()}')

  prop1 = []
  prop2 = []
  prop3 = []
  for row in datastore:
    prop1.append(row['prop1'])
    prop2.append(row['prop2'])
    prop3.append(row['prop3'])
```

## load CSV data into Python:

```py
import csv

with open('data.csv', 'r') as f:
  datastore = csv.reader(csvfile, delimiter=',')
  # print(f'First line" {f.readline()}')
  # print(f'Second line" {f.readline()}')

  prop1 = []
  prop2 = []
  prop3 = []
  for row in datastore:
    prop1.append(row['prop1'])
    prop2.append(row['prop2'])
    prop3.append(row['prop3'])
```

```py
# hyperparameters:
vocab_size = 100
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'
num_epochs = 10

sentences = ['I love my dog', 'I love my cat']

import tensorflow as tf
from tf import keras
from tf.keras.preprocessing.text import Tokenizer
from tf.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(
  num_words=vocab_size, # use top x most-common words to tokenize SENTENCES for tons of data
  oov_token=oov_tok, # mark not-yet-seen words as OOV = Out Of Vocabulary
)

tokenizer.fit_on_texts(sentences)

word_index = tokenizer.word_index # dictionary key-value pairs (may have len > than num_words)
print(word_index) # {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}

sequences = tokenizer.texts_to_sequences(sentences)
print(sequences) # [[1,2,3,4], [1,2,3,5]]

padded = pad_sequences( # add 0s to make inputs of uniform size
  sequences,
  padding='post', # use 'post' to add padding to end of sentence (default is 'pre')
  # maxlen=max_length, # max number of words to include in encoding (else all len of longest)
  # truncating=trunc_type # truncate from end of sentence if there too many words in sentence
)
# do similar for test: test_padded = pad_sequences(...)
test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=Max_length)
```

## create a model with an embedding layer!

```py
model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  # tf.keras.layers.Flatten(), # may be slower but more accurate than:
  tf.keras.layers.GlobalAveragePooling1D(), # flatten by averaging across the vector
  tf.keras.layers.Dense(6, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
history = model.fit(
  padded,
  train_labels_final,
  epochs=num_epochs,
  validation_data=(test_padded, test_labels_final)
)
```

```py
import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_' + string])
  plt.xlabel('Epochs')
  plt.ylabel(string)
  plt.legend([string, 'val_' + string])
  plt.show()

plot_graphs(history, 'acc')
plot_graphs(history, 'loss')
```

Note: often for text data, you might notice validation (AKA test) accuracy increase but its loss increase = slowly more accurate predictions over time but confidence per prediction decreased. Try playing with hyperparameters to see what performs best.

## visualize the model's embeddings

```py
embedding_layer = model.layers[0] # first layer as per model above
weights = embedding_layer.get_weights()
print(weights.shape) # shape: (vocab_size, embedding_dim), e.g. (10000, 16)
print(tokenizer.word_index) # e.g., {word: 1, ...}
print(tokenizer.index_word) # e.g., {1: word, ...}
index_word = tokenizer.index_word

import io
# vectors = embeddings for each word
# words = "metadata" (labels/words represented by those embeddings)
vectors = io.open('vectors.tsv', 'w', encoding='utf-8')
words = io.open('words.tsv', 'w', encoding='utf-8')
for word_number_key in range(1, vocab_size):
  word = index_word[word_number_key] # index_word as opposed to word_index
  embeddings = weights[word_number_key]
  words.write(word + '\n')
  vectors.write('\t'.join([str(x) for x in embeddings]) + '\n')
vectors.close()
words.close()

# # to download the files from Google Colab to your computer:
# try:
#   from google.colab import files
# except ImportError:
#   pass
# else:
#   files.download('vectors.tsv')
#   files.download('words.tsv')
```

## to then visualize those exported .tsv files:

https://projector.tensorflow.org/ --> Load data --> Sphereize data (for binary clustering of the data)
